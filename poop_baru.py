# -*- coding: utf-8 -*-
"""poop_baru.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r5cpj5-gYiMVUgp76TvmcECPOep4jyIe

# INPUT LINK.txt nya
"""

from google.colab import widgets
import ipywidgets as widgets
from IPython.display import display

# Buat textarea untuk memasukkan link
textarea = widgets.Textarea(
    placeholder="Masukkan link di sini...",
    layout=widgets.Layout(width='100%', height='200px')
)

# Buat tombol simpan
button = widgets.Button(description="Simpan ke link.txt")
output = widgets.Output()

# Fungsi untuk menyimpan input ke file dan menampilkan isi file
def save_to_file(b):
    with open("link.txt", "w") as file:
        file.write(textarea.value.strip() + "\n")  # Tambahkan newline

    # Tampilkan pesan sukses dan isi file
    with output:
        output.clear_output()  # Hapus output sebelumnya
        print("✅ Link berhasil disimpan ke link.txt!\n")

        # Baca dan tampilkan isi file
        with open("link.txt", "r") as file:
            print("Isi dari link.txt:\n")
            print(file.read())

# Hubungkan tombol ke fungsi
button.on_click(save_to_file)

# Tampilkan elemen di Colab
display(textarea, button, output)

"""# ISI output_link.txt"""

import os
from IPython.core.display import display, HTML

# Cek apakah file ada
file_path = "output_link.txt"
if os.path.exists(file_path):
    with open(file_path, "r") as file:
        content = file.read().strip()
else:
    content = "File output_link.txt tidak ditemukan."

# Hitung jumlah baris (link)
num_lines = content.count("\n") + 1 if content else 0

# Buat elemen HTML dengan tombol Copy & jumlah link
html_content = f"""
<h4>Isi dari output_link.txt:</h4>
<textarea id="linkArea" rows="10" style="width:100%;">{content}</textarea><br>
<button onclick="copyAll()">Copy Semua</button>
<p id="countText">Jumlah link: {num_lines}</p>

<script>
function copyAll() {{
    let textArea = document.getElementById("linkArea");
    textArea.select();
    document.execCommand("copy");
    alert("Berhasil menyalin semua link!");
}}
</script>
"""

# Tampilkan di Colab
display(HTML(html_content))

"""isi Link.txt"""

import os
from IPython.core.display import display, HTML

# Cek apakah file ada
file_path = "link.txt"
if os.path.exists(file_path):
    with open(file_path, "r") as file:
        content = file.read().strip()
else:
    content = "File link.txt tidak ditemukan."

# Hitung jumlah baris (link)
num_lines = content.count("\n") + 1 if content else 0

# Buat elemen HTML dengan tombol Copy & jumlah link
html_content = f"""
<h4>Isi dari link.txt:</h4>
<textarea id="linkArea" rows="10" style="width:100%;">{content}</textarea><br>
<button onclick="copyAll()">Copy Semua</button>
<p id="countText">Jumlah link: {num_lines}</p>

<script>
function copyAll() {{
    let textArea = document.getElementById("linkArea");
    textArea.select();
    document.execCommand("copy");
    alert("Berhasil menyalin semua link!");
}}
</script>
"""

# Tampilkan di Colab
display(HTML(html_content))

# Install dependensi di Google Colab
!pip install pycryptodome beautifulsoup4 requests

# Buat file kosong jika belum ada
with open("link.txt", "w") as f:
    pass

with open("output_link.txt", "w") as f:
    pass


# Remove the now empty directory
!rm -r *
# Clone the repository
# Step 2: Clone the repository
!git clone https://github.com/muslimfact/poopbaru/

# Step 3: Move contents to current directory
!mv poopbaru/* ./
!mv poopbaru/.* ./  # This moves hidden files (if any)



print("Instalasi selesai & file sudah dibuat.")

"""# Jalan /d/"""

!python poop_jalan.py

"""# jalan /f/"""

!python poop_jalan_folder.py

"""# ***TurboStream UPLOAD"""

# ⬅️ Pastikan semua di satu sel di Colab

import httpx
import ipywidgets as widgets
from IPython.display import display
import json

# API key dan endpoint
turbostream_api_key = "98a5744ffbd3c2a63e0f61a37b3661f0f73b16b281da8fda8ade989dd93e76fe"
turbostream_api_endpoint = "https://turbostream.tv/api/remote_upload.php"

# Buat textarea dan tombol
textarea = widgets.Textarea(
      placeholder='Input 1 URL per baris',
    description='URLs:',
    layout=widgets.Layout(width='100%', height='200px')
)

button = widgets.Button(description="Proses")

output = widgets.Output()

# Fungsi saat tombol diklik
def on_button_clicked(b):
    output.clear_output()
    urls = textarea.value.strip().splitlines()
    success_count = 0

    with output:
        for url in urls:
            url = url.strip()
            if not url:
                continue
            try:
                headers = {
                    "Authorization": f"Bearer {turbostream_api_key}",
                    "Content-Type": "application/json"
                }

                payload = {
                    "url": url
                }

                response = httpx.post(turbostream_api_endpoint, headers=headers, json=payload)

                if response.status_code == 200:
                    success_count += 1
                    print(f"[✅] Success: {url}")
                else:
                    print(f"[❌] Failed: {url} - {response.status_code} - {response.text}")

            except Exception as e:
                print(f"[⚠️] Error: {url} - {e}")

        print(f"\nTotal Berhasil: {success_count}/{len(urls)}")

button.on_click(on_button_clicked)

# Tampilkan UI
display(textarea, button, output)

"""==== GRABNWATCH ============================="""

!apt-get update
!apt install -y chromium-chromedriver
!pip install selenium webdriver-manager

!git clone https://github.com/muslimfact/poopbaru/

"""# ===== Doodstream Exract /f/ folder disimpan di link.txt"""

import requests
from bs4 import BeautifulSoup
from IPython.display import display, HTML, Image, clear_output
import ipywidgets as widgets

# Fungsi untuk melakukan scraping
def scrape_urls(urls):
    total_links = 0
    processed_urls = 0

    # Buka file untuk menulis semua link
    with open('link.txt', 'w') as file, open('output_link.txt', 'w') as output_file:
        for url in urls:
            url = url.strip()
            if not url:
                continue

            try:
                # Kirim request GET ke URL
                response = requests.get(url)
                soup = BeautifulSoup(response.content, 'html.parser')

                # Temukan container dengan data
                container = soup.find('div', class_='the_box search-files')
                if not container:
                    continue

                # Loop melalui setiap item
                for item in container.find_all('li'):
                    # Dapatkan judul
                    title = item.find('h4').text.strip() if item.find('h4') else "No Title"

                    # Dapatkan sumber gambar
                    img_tag = item.find('img')
                    img_src = img_tag['src'] if img_tag else ''
                    if img_src.startswith('//'):
                        img_src = 'https:' + img_src

                    # Dapatkan link
                    link = item.find('a')['href'] if item.find('a') else ''

                    # Tulis link ke kedua file
                    file.write(link + '\n')
                    output_file.write(link + '\n')

                    # Tambah counter
                    total_links += 1

                    # Tampilkan di Colab
                    display(HTML(f"""
                    <div style="margin-bottom: 20px; border: 1px solid #ddd; padding: 10px; border-radius: 5px;">
                        <h4>{title}</h4>
                        <img src="{img_src}" style="max-width: 300px; max-height: 200px;" onerror="this.style.display='none'">
                        <div><a href="{link}" target="_blank">{link}</a></div>
                    </div>
                    """))

                processed_urls += 1

            except Exception as e:
                display(HTML(f'<div style="color: red;">Error processing {url}: {str(e)}</div>'))

    # Tampilkan total
    clear_output(wait=True)
    display(HTML(f"<h2 style='color: white; background-color: green; padding: 10px;'>PROSES SELESAI!</h2>"))
    display(HTML(f"<h3 style='color: white;'>Total URL diproses: {processed_urls}</h3>"))
    display(HTML(f"<h3 style='color: white;'>TOTAL LINK DITEMUKAN: {total_links}</h3>"))

# UI Widgets
text_area = widgets.Textarea(
    value='',
    placeholder='Masukkan URL, ',
    description='URLs:',
    layout={'width': '80%', 'height': '150px'}
)

button = widgets.Button(
    description='Proses URL',
    button_style='success',
    tooltip='Klik untuk memulai scraping'
)

output = widgets.Output()

def on_button_clicked(b):
    with output:
        clear_output()
        urls = text_area.value.split('\n')
        scrape_urls(urls)

button.on_click(on_button_clicked)

# Tampilkan UI
display(HTML("<h2 style='color: white;'>MASUKKAN URL UNTUK DI-SCRAPE</h2>"))
display(text_area)
display(button)
display(output)

"""# ==== input link Dood Khusus /d/ doang"""

# Import library yang dibutuhkan
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import ipywidgets as widgets
from IPython.display import display, Javascript

# === BLOK 1: Simpan inputan ke link.txt ===
# Input textarea
links_input = widgets.Textarea(
    placeholder="Masukkan daftar link, satu per baris...",
    layout=widgets.Layout(width='100%', height='150px')
)

# Tombol untuk menyimpan ke file
save_button = widgets.Button(description="Simpan ke link.txt", button_style='success')

# Fungsi menyimpan input ke file
def save_links(b):
    # Ambil teks dari input, pisahkan per baris, dan hapus spasi ekstra
    cleaned_links = "\n".join(line.strip() for line in links_input.value.splitlines() if line.strip())

    # Simpan ke file
    with open("link.txt", "w") as f:
        f.write(cleaned_links)

    print("Link tersimpan ke link.txt!")

    # Cetak ulang isi file tanpa spasi tambahan
    print("\nIsi link.txt setelah disimpan:")
    print(cleaned_links)  # Langsung cetak string yang sudah dibersihkan

save_button.on_click(save_links)

# Tampilkan UI untuk input dan tombol simpan
display(links_input, save_button)

"""# ========= Proses KHUSUS xhamster, Eporner, Redtube"""

# === BLOK 2: Membaca link.txt dan menjalankan scraping ===
# Output textarea
output_area = widgets.Textarea(
    value="",
    placeholder="Hasil akan muncul di sini...",
    layout=widgets.Layout(width='100%', height='150px'),
    disabled=True
)

# Tombol untuk scraping
copy_button = widgets.Button(description="Copy Hasil", button_style='info')

# Konfigurasi WebDriver
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--no-sandbox")

chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

driver = webdriver.Chrome(options=chrome_options)

# Tunggu hasilnya muncul
def get_resolution_value(button_text):
  # Bersihkan teks dan ubah ke lowercase
  text = button_text.strip().lower()

  # Cari semua angka dalam teks
  numbers = []
  current_num = ''
  for char in text:
      if char.isdigit():
          current_num += char
      elif current_num:
          numbers.append(int(current_num))
          current_num = ''

  # Tambahkan angka terakhir jika ada
  if current_num:
      numbers.append(int(current_num))

  # Kembalikan angka terbesar yang ditemukan, atau 0 jika tidak ada angka
  return max(numbers) if numbers else 0

# Fungsi scraping
def scrape_links(b=None):
    try:
        with open("link.txt", "r") as f:
            links = f.read().strip().split("\n")

        total_links = len(links)
        results = []

        for index, link in enumerate(links):
            print(f"Processing {index + 1} dari {total_links}: {link}")
            driver.get('https://grabnwatch.com/')

            try:
                WebDriverWait(driver, 20).until(
                  EC.invisibility_of_element_located((By.ID, 'checkingMessage'))
                  )
                # Tunggu input muncul
                input_box = WebDriverWait(driver, 20).until(
                    EC.presence_of_element_located((By.NAME, 'video_url'))
                )
                input_box.clear()
                input_box.send_keys(link)

                # Klik tombol grab
                grab_button = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), "Grab")]'))
                )
                grab_button.click()

                # Tunggu hasilnya muncul
                download_buttons = WebDriverWait(driver, 10).until(
                    EC.presence_of_all_elements_located((By.CLASS_NAME, 'btn-secondary'))
                )

                # Cari tombol dengan resolusi tertinggi
                highest_res_button = None
                max_resolution = 0

                for button in download_buttons:
                    try:
                        current_res = get_resolution_value(button.text)

                        # Debug: print untuk melihat hasil parsing
                        print(f"Button text: '{button.text}' → Res: {current_res}")

                        if current_res > max_resolution:
                            max_resolution = current_res
                            highest_res_button = button
                    except Exception as e:
                        print(f"Error processing button '{button.text}': {e}")
                        continue

                # Tentukan link yang akan dipakai
                if highest_res_button:
                    download_link = highest_res_button.get_attribute('href')
                    print(f"=>> Selected highest resolution ({max_resolution}): {download_link}")
                    results.append(download_link)
                else:
                    # Fallback: ambil tombol pertama jika tidak ada yang memenuhi kriteria
                    download_link = download_buttons[0].get_attribute('href') if download_buttons else None
                    print("=>> No valid resolution found, using fallback")
                    results.append(download_link)


            except Exception as e:
                print(f"Error: {link} gagal diproses. Error: {e}")


        # Tampilkan hasil di textarea output
        output_area.value = "\n".join(results)

    except FileNotFoundError:
        print("File link.txt tidak ditemukan. Simpan link terlebih dahulu.")

# Fungsi untuk copy hasil
def copy_to_clipboard(b):
    display(Javascript("""
    var text = document.querySelector('textarea[disabled]').value;
    navigator.clipboard.writeText(text).then(() => {
        alert('Hasil disalin ke clipboard!');
    });
    """))

# Hubungkan tombol ke fungsi
scrape_links()
copy_button.on_click(copy_to_clipboard)

# Tampilkan UI untuk scraping dan output
display(output_area, copy_button)

"""# XVIDEOS Khusus"""

# === BLOK 2: Membaca link.txt dan menjalankan scraping ===
# Output textarea
output_area = widgets.Textarea(
    value="",
    placeholder="Hasil akan muncul di sini...",
    layout=widgets.Layout(width='100%', height='150px'),
    disabled=True
)

# Tombol untuk scraping
copy_button = widgets.Button(description="Copy Hasil", button_style='info')

# Konfigurasi WebDriver
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--no-sandbox")

chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

driver = webdriver.Chrome(options=chrome_options)

# Fungsi scraping
def scrape_links(b=None):
    try:
        with open("link.txt", "r") as f:
            links = f.read().strip().split("\n")

        total_links = len(links)
        results = []

        for index, link in enumerate(links):
            print(f"Processing {index + 1} dari {total_links}: {link}")
            driver.get('https://grabnwatch.com/')

            try:
                WebDriverWait(driver, 20).until(
                  EC.invisibility_of_element_located((By.ID, 'checkingMessage'))
                  )
                # Tunggu input muncul
                input_box = WebDriverWait(driver, 20).until(
                    EC.presence_of_element_located((By.NAME, 'video_url'))
                )
                input_box.clear()
                input_box.send_keys(link)

                # Klik tombol grab
                grab_button = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), "Grab")]'))
                )
                grab_button.click()

                # Tunggu hasilnya muncul
                download_buttons = WebDriverWait(driver, 10).until(
                    EC.presence_of_all_elements_located((By.CLASS_NAME, 'btn-secondary'))
                )

                # Tampilkan semua tombol yang ditemukan
                print("Tombol ditemukan:")
                for btn in download_buttons:
                    print(f"- {btn.text.strip()}")

                # Cari tombol dengan teks 'High quality'
                high_quality = None
                for btn in download_buttons:
                    if "High quality" in btn.text:
                        high_quality = btn.get_attribute("href")
                        break

                # Jika tidak ada 'High quality', ambil tombol pertama
                download_link = high_quality if high_quality else download_buttons[0].get_attribute("href")
                results.append(download_link)
                print(f"=>> Download : {download_link}")


            except Exception as e:
                print(f"Error: {link} gagal diproses. Error: {e}")


        # Tampilkan hasil di textarea output
        output_area.value = "\n".join(results)
        with open("output_link.txt", "w") as output_file:
            for result in results:
                output_file.write(result + "\n")

    except FileNotFoundError:
        print("File link.txt tidak ditemukan. Simpan link terlebih dahulu.")

# Fungsi untuk copy hasil
def copy_to_clipboard(b):
    display(Javascript("""
    var text = document.querySelector('textarea[disabled]').value;
    navigator.clipboard.writeText(text).then(() => {
        alert('Hasil disalin ke clipboard!');
    });
    """))

# Hubungkan tombol ke fungsi
scrape_links()
copy_button.on_click(copy_to_clipboard)

# Tampilkan UI untuk scraping dan output
display(output_area, copy_button)

"""# ==== KHUSUS JALAN DOODSTREAM POOPHD"""

# === BLOK 2: Membaca link.txt dan menjalankan scraping ===
# Output textarea
output_area = widgets.Textarea(
    value="",
    placeholder="Hasil akan muncul di sini...",
    layout=widgets.Layout(width='100%', height='150px'),
    disabled=True
)

# Tombol untuk scraping
copy_button = widgets.Button(description="Copy Hasil", button_style='info')

# Konfigurasi WebDriver
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument("--headless")
chrome_options.add_argument("--no-sandbox")

chrome_options.add_argument("--disable-dev-shm-usage")
chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

driver = webdriver.Chrome(options=chrome_options)

# Fungsi scraping
def scrape_links(b=None):
    try:
        with open("link.txt", "r") as f:
            links = f.read().strip().split("\n")

        total_links = len(links)
        results = []

        for index, link in enumerate(links):
            print(f"Processing {index + 1} dari {total_links}: {link}")
            driver.get('https://grabnwatch.com/')

            try:
                WebDriverWait(driver, 20).until(
                  EC.invisibility_of_element_located((By.ID, 'checkingMessage'))
                  )
                # Tunggu input muncul
                input_box = WebDriverWait(driver, 20).until(
                    EC.presence_of_element_located((By.NAME, 'video_url'))
                )
                input_box.clear()
                input_box.send_keys(link)

                # Klik tombol grab
                grab_button = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.XPATH, '//button[contains(text(), "Grab")]'))
                )
                grab_button.click()

                # Tunggu hasilnya muncul
                download_link = WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.CLASS_NAME, 'btn-secondary'))
                ).get_attribute('href')

                results.append(download_link)
                print(f"=>> Download : {download_link}")

            except Exception as e:
                print(f"Error: {link} gagal diproses. Error: {e}")


        # Tampilkan hasil di textarea output
        output_area.value = "\n".join(results)
        with open("output_link.txt", "w") as output_file:
            for result in results:
                output_file.write(result + "\n")

    except FileNotFoundError:
        print("File link.txt tidak ditemukan. Simpan link terlebih dahulu.")

# Fungsi untuk copy hasil
def copy_to_clipboard(b):
    display(Javascript("""
    var text = document.querySelector('textarea[disabled]').value;
    navigator.clipboard.writeText(text).then(() => {
        alert('Hasil disalin ke clipboard!');
    });
    """))

# Hubungkan tombol ke fungsi
scrape_links()
copy_button.on_click(copy_to_clipboard)

# Tampilkan UI untuk scraping dan output
display(output_area, copy_button)

!python remote_upload.py

"""# ==== ## JAVHAT SCRAPE"""

import requests
from bs4 import BeautifulSoup
import json
import base64
from IPython.display import display, HTML

def scrape_video_links(main_url):
    print(f"page : {main_url}")
    try:
        # Lakukan request ke halaman utama
        response = requests.get(main_url, headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept-Language": "en-US,en;q=0.9"
        })
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # Temukan div panel-body yang berisi video
        panel_body = soup.find('div', class_='panel-body panel-padding')
        if not panel_body:
            raise Exception("Panel body not found")

        # Temukan semua elemen li dengan id yang dimulai dengan 'video-'
        video_items = panel_body.find_all('li', id=lambda x: x and x.startswith('video-'))

        videos_data = []

        for item in video_items:
            try:
                # Temukan elemen a dengan class thumbnail
                a_tag = item.find('a', class_='thumbnail')

                if a_tag:
                    # Ambil href dan title dari a tag
                    href = a_tag.get('href', '')
                    title = a_tag.get('title', '')

                    # Hapus prefix proxy jika ada
                    if href.startswith('/proxy/'):
                        href = href[7:]
                    if not href.startswith('http'):
                        href = f"https://javhat.tv{href}"

                    # Scrape detail halaman untuk mendapatkan link video
                    video_links = scrape_video_page(href)

                    # Temukan img tag
                    img_tag = a_tag.find('img')
                    img_src = img_tag.get('src', '') if img_tag else ''
                    if img_src.startswith('/proxy/'):
                        img_src = img_src[7:]
                    if img_src and not img_src.startswith('http'):
                        img_src = f"https://javhat.tv{img_src}"

                    # Temukan rating
                    rating_span = a_tag.find('span', class_='video-rating')
                    rating = rating_span.get_text(strip=True) if rating_span else ''

                    # Temukan tanggal
                    date_span = a_tag.find('span', class_='video-overlay')
                    date = date_span.get_text(strip=True) if date_span else ''

                    # Tambahkan ke dictionary
                    video_data = {
                        'title': title,
                        'href': href,
                        'img_src': img_src,
                        'rating': rating,
                        'date': date,
                        'video_links': video_links
                    }

                    videos_data.append(video_data)

            except Exception as e:
                print(f"Error saat memproses item video {href}: {e}")
                continue

        return videos_data

    except Exception as e:
        print(f"Error saat scraping halaman utama: {e}")
        return []

def scrape_video_page(url):
    print(f"== >  process : {url}")
    try:
        # Gunakan proxy untuk mengakses halaman detail
        unblock = "https://darenx-unblockjapan.hf.space/proxy"
        proxy_url = f"{unblock}/{url}"

        response = requests.get(proxy_url, headers={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        })
        response.raise_for_status()

        soup = BeautifulSoup(response.content, 'html.parser')

        # Temukan div button_style
        button_style = soup.find('div', class_='button_style')
        if not button_style:
            return []

        # Temukan semua button choice server
        buttons = button_style.find_all('button', class_='button_choice_server')

        video_links = []

        for button in buttons:
            try:
                # Ekstrak onclick attribute
                onclick = button.get('onclick', '')
                if not onclick:
                    continue

                # Ekstrak encoded URL dari onclick
                # Format: selectServer(this, atob('encoded_url'))
                start = onclick.find("atob('") + 6
                end = onclick.find("')", start)
                encoded_url = onclick[start:end]

                # Decode base64
                decoded_url = base64.b64decode(encoded_url).decode('utf-8')

                video_links.append(decoded_url)

            except Exception as e:
                print(f"Error memproses button di {url}: {e}")
                continue

        return video_links

    except Exception as e:
        print(f"Error saat scraping halaman video {url}: {e}")
        return []

# URL utama
unblock = "https://darenx-unblockjapan.hf.space/proxy"
mau_page = input("masukkan page berapa : ")
main_url = f"{unblock}/https://javhat.tv/recent/{mau_page}/"

# Jalankan scraping
videos_data = scrape_video_links(main_url)

# Tampilkan hasil dalam format HTML yang rapi
html_output = ""
video_links_text = ""
for video in videos_data:
    html_output += f"""
    <div style="border: 1px solid #ddd; padding: 10px; margin-bottom: 10px;">
        <img src="{video['img_src']}" alt="{video['title']}" style="max-width: 100%; height: auto;">
        <h3>{video['title']}</h3>
        <p><strong>Rating:</strong> {video['rating']}</p>
        <p><strong>Date:</strong> {video['date']}</p>
        <p><strong>URL:</strong> <a href="{video['href']}" target="_blank">{video['href']}</a></p>
        <p><strong>Video Links:</strong></p>
        <ul>
    """
    for link in video['video_links']:
        html_output += f"<li><a href='{link}' target='_blank'>{link}</a></li>"
        video_links_text += f"{link}\n"
    html_output += "</ul></div>"

# Tambahkan textarea dan tombol copy
html_output += f"""
<div style="margin-top: 20px;">
    <textarea id="videoLinksTextarea" rows="10" cols="100" style="width: 100%;">{video_links_text}</textarea>
    <br>
    <button onclick="copyVideoLinks()">Copy</button>
    <script>
        function copyVideoLinks() {{
            var textarea = document.getElementById('videoLinksTextarea');
            textarea.select();
            document.execCommand('copy');
            alert('Berhasil di-copy!');
        }}
    </script>
</div>
"""

display(HTML(html_output))

"""# ===== # === scrape lanjutan Watchvideo javhat"""

import requests
from bs4 import BeautifulSoup
from IPython.display import display, HTML

# URL utama

process_domain = "https://watchvideo.javhat.tv"

# Contoh data video (dapat diambil dari hasil sebelumnya)


# Filter link berdasarkan domain
filtered_links = [link for v in videos_data for link in v['video_links'] if link.startswith(process_domain)]

# List untuk menyimpan semua link playEmbed
all_play_embed_links = []

# Scrape data dari setiap link yang telah difilter
for link in filtered_links:
    print(link)
    response = requests.get(link)
    soup = BeautifulSoup(response.content, 'html.parser')

    # Temukan elemen dengan class 'btn-group'
    btn_group = soup.find('div', class_='btn-group')
    if btn_group:
        # Ambil semua elemen <li> dengan class 'button_choice_server'
        button_choice_servers = btn_group.find_all('li', class_='button_choice_server')
        for button in button_choice_servers:
            # Ambil URL dari onclick
            onclick_attr = button.get('onclick')
            if onclick_attr:
                url = onclick_attr.split("'")[1]  # Ambil URL dari onclick
                all_play_embed_links.append(url)  # Menyimpan URL ke list

# Tampilkan semua link dalam textarea
links_text = "\n".join(all_play_embed_links)

# Tampilkan textarea dan tombol copy
display(HTML(f"""
    <h4>{main_url}</h4>
    <textarea id="result" rows="10" cols="50">{links_text}</textarea>
    <br>
    <button onclick="copyToClipboard()">Copy</button>
    <script>
        function copyToClipboard() {{
            var copyText = document.getElementById("result");
            copyText.select();
            document.execCommand("copy");
            alert("Copied the text: " + copyText.value);
        }}
    </script>
"""))